{"appId":"winget_app_Mozilla.llamafile","appShortcutName":"Winget Application","appDisplayName":"llamafile","authorId":"winget","releaseTagName":"winget-0.8.14","downloadUrls":{"1":{"installerType":"WindowsInstallerExe","asset":"","url":"https://github.com/Mozilla-Ocho/llamafile/releases/download/0.8.14/llamafile-0.8.14"},"0":{"installerType":"WindowsInstallerMsi","asset":"","url":""}},"install":{"win32":{"assetId":1,"exec":null,"installerArgs":null},"winarm":null,"linux":null,"linuxArm64":null,"linuxArm7":null,"android":null},"displayImages":[],"description":"Distribute and run LLMs with a single file.\n\nllamafile lets you distribute and run LLMs with a single file.\nOur goal is to make open LLMs much more accessible to both developers and end users. We're doing that by combining llama.cpp with Cosmopolitan Libc into one framework that collapses all the complexity of LLMs down to a single-file executable (called a \"llamafile\") that runs locally on most computers, with no installation.\nThis release introduces our new CLI chatbot interface. It supports multi-line input using triple quotes. It will syntax highlight Python, C, C++, Java, and JavaScript code.\nThis chatbot is now the default mode of operation. When you launch llamafile without any special arguments, the chatbot will be launched in the foreground, and the server will be launched in the background. You can use the --chat and --server flags to disambiguate this behavior if you only want one of them.\n- a384fd7 Create ollama inspired cli chatbot\n- 63205ee Add syntax highlighting to chatbot\n- 7b395be Introduce new --chat flag for chatbot\n- 28e98b6 Show prompt loading progress in chatbot\n- 4199dae Make chat+server hybrid the new default mode\nThe whisperfile server now lets you upload mp3/ogg/flac.\n- 74dfd21 Rewrite audio file loader code\n- 7517a5f whisperfile server: convert files without ffmpeg (#568)\nOther improvements have been made.\n- d617c0b Added vision support to api_like_OAI (#524)\n- 726f6e8 Enable gpu support in llamafile-bench (#581)\n- c7c4d65 Speed up KV in llamafile-bench\n- 2c940da Make replace_all() have linear complexity\n- fa4c4e7 Use bf16 kv cache when it's faster\n- 20fe696 Upgrade to Cosmopolitan 3.9.4\n- c44664b Always favor fp16 arithmetic in tinyBLAS\n- 98eff09 Quantize TriLM models using Q2_K_S (#552)","repo":{"author":"microsoft","repo":"winget-pkgs"},"version":"0.8.14","site":"https://github.com/Mozilla-Ocho","source":"WinGet","license_or_tos":"Apache-2.0","resources":null,"verified":false}
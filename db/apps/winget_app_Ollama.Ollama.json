{"appId":"winget_app_Ollama.Ollama","appShortcutName":"Winget Application","appDisplayName":"Ollama","authorId":"winget","releaseTagName":"winget-0.3.14","downloadUrls":{"1":{"installerType":"WindowsInstallerExe","asset":"","url":"https://github.com/ollama/ollama/releases/download/v0.3.14/OllamaSetup.exe"},"0":{"installerType":"WindowsInstallerMsi","asset":"","url":""}},"install":{"win32":{"assetId":1,"exec":null,"installerArgs":null},"winarm":null,"linux":null,"linuxArm64":null,"linuxArm7":null,"android":null},"displayImages":[],"description":"Get up and running with large language models locally.\n\n\nNew Models\n- Granite 3 MoE: The IBM Granite 1B and 3B models are the first mixture of experts (MoE) Granite models from IBM designed for low latency usage.\n- Granite 3 Dense: The IBM Granite 2B and 8B models are designed to support tool-based use cases and support for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing.\nThank you @gabe-l-hart for contributing Granite support to Ollama!\nWhat's Changed\n- Fix crashes for AMD GPUs with small system memory\n- Fix error that would occur on macOS 11 Big Sur\n- Fixed issue creating models from bf16 file types\n- Improve CPU performance by improving default thread counts\n- IBM granite/granitemoe architecture support by @gabe-l-hart in https://github.com/ollama/ollama/pull/6760\nNew Contributors\n- @JHubi1 made their first contribution in https://github.com/ollama/ollama/pull/6465\n- @gabe-l-hart made their first contribution in https://github.com/ollama/ollama/pull/6760\nFull Changelog: https://github.com/ollama/ollama/compare/v0.3.13...v0.3.14","repo":{"author":"microsoft","repo":"winget-pkgs"},"version":"0.3.14","site":"https://ollama.com/","source":"WinGet","license_or_tos":"MIT","resources":null,"verified":false}
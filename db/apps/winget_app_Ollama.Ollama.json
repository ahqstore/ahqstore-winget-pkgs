{"appId":"winget_app_Ollama.Ollama","appShortcutName":"Winget Application","appDisplayName":"Ollama","authorId":"winget","releaseTagName":"winget-0.5.0","downloadUrls":{"0":{"installerType":"WindowsInstallerMsi","asset":"","url":""},"1":{"installerType":"WindowsInstallerExe","asset":"","url":"https://github.com/ollama/ollama/releases/download/v0.5.0/OllamaSetup.exe"}},"install":{"win32":{"assetId":1,"exec":null,"scope":"User","installerArgs":null},"winarm":null,"linux":null,"linuxArm64":null,"linuxArm7":null,"android":null},"displayImages":[],"description":"Get up and running with large language models locally.\n\n\nStructured outputs\nOllama now supports structured outputs, making it possible to constrain a model's output to a specific format defined by a JSON schema. The Ollama Python and JavaScript libraries have been updated to support structured outputs, together with Ollama's OpenAI-compatible API endpoints.\nREST API\nTo use structured outputs in Ollama's generate or chat APIs, provide a JSON schema object in the format parameter:\ncurl -X POST http://localhost:11434/api/chat -H \"Content-Type: application/json\" -d '{\n  \"model\": \"llama3.1\",\n  \"messages\": [{\"role\": \"user\", \"content\": \"Tell me about Canada.\"}],\n  \"stream\": false,\n  \"format\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"name\": {\n        \"type\": \"string\"\n      },\n      \"capital\": {\n        \"type\": \"string\"\n      },\n      \"languages\": {\n        \"type\": \"array\",\n        \"items\": {\n          \"type\": \"string\"\n        }\n      }\n    },\n    \"required\": [\n      \"name\",\n      \"capital\",\n      \"languages\"\n    ]\n  }\n}'\nPython library\nUsing the Ollama Python library, pass in the schema as a JSON object to the format parameter as either dict or use Pydantic (recommended) to serialize the schema using model_json_schema().\nfrom ollama import chat\nfrom pydantic import BaseModel\n\nclass Country(BaseModel):\n  name: str\n  capital: str\n  languages: list[str]\n\nresponse = chat(\n  messages=[\n    {\n      'role': 'user',\n      'content': 'Tell me about Canada.',\n    }\n  ],\n  model='llama3.1',\n  format=Country.model_json_schema(),\n)\n\ncountry = Country.model_validate_json(response.message.content)\nprint(country)\nJavaScript library\nUsing the Ollama JavaScript library, pass in the schema as a JSON object to the format parameter as either object or use Zod (recommended) to serialize the schema using zodToJsonSchema():\nimport ollama from 'ollama';\nimport { z } from 'zod';\nimport { zodToJsonSchema } from 'zod-to-json-schema';\n\nconst Country = z.object({\n    name: z.string(),\n    capital: z.string(),\n    languages: z.array(z.string()),\n});\n\nconst response = await ollama.chat({\n    model: 'llama3.1',\n    messages: [{ role: 'user', content: 'Tell me about Canada.' }],\n    format: zodToJsonSchema(Country),\n});\n\nconst country = Country.parse(JSON.parse(response.message.content));\nconsole.log(country);\nWhat's Changed\n- Fixed error importing model vocabulary files\n- Experimental: new flag to set KV cache quantization to 4-bit (q4_0), 8-bit (q8_0) or 16-bit (f16). This reduces VRAM requirements for longer context windows.\n  - To enable for all models, use OLLAMA_FLASH_ATTENTION=1 OLLAMA_KV_CACHE_TYPE=q4_0 ollama serve\n  - Note: in the future flash attention will be enabled by default where available, with kv cache quantization available on a per-model basis\n  - Thank you @sammcj for the contribution in in https://github.com/ollama/ollama/pull/7926\nNew Contributors\n- @dmayboroda made their first contribution in https://github.com/ollama/ollama/pull/7906\n- @Geometrein made their first contribution in https://github.com/ollama/ollama/pull/7908\n- @owboson made their first contribution in https://github.com/ollama/ollama/pull/7693\nFull Changelog: https://github.com/ollama/ollama/compare/v0.4.7...v0.5.0","repo":{"author":"microsoft","repo":"winget-pkgs"},"version":"0.5.0","site":"https://ollama.com/","source":"Ollama","license_or_tos":"MIT","resources":null,"verified":false}
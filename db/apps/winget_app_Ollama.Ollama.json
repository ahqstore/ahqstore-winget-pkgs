{"appId":"winget_app_Ollama.Ollama","appShortcutName":"Winget Application","appDisplayName":"Ollama","authorId":"winget","releaseTagName":"winget-0.4.1","downloadUrls":{"0":{"installerType":"WindowsInstallerMsi","asset":"","url":""},"1":{"installerType":"WindowsInstallerExe","asset":"","url":"https://github.com/ollama/ollama/releases/download/v0.4.1/OllamaSetup.exe"}},"install":{"win32":{"assetId":1,"exec":null,"installerArgs":null},"winarm":null,"linux":null,"linuxArm64":null,"linuxArm7":null,"android":null},"displayImages":[],"description":"Get up and running with large language models locally.\n\n\nWhat's Changed\n- Fixed an issue where the cuda v12 runner on linux was incorrectly linked\n- Fixed an issue with multiple AMD GPUs on Windows resulting in gibberish output\n- Fixed support for MacOS v11 on ARM systems\nNew Contributors\n- @edmcman made their first contribution in https://github.com/ollama/ollama/pull/7527\nFull Changelog: https://github.com/ollama/ollama/compare/v0.4.0...v0.4.1-rc0","repo":{"author":"microsoft","repo":"winget-pkgs"},"version":"0.4.1","site":"https://ollama.com/","source":"Ollama","license_or_tos":"MIT","resources":null,"verified":false}
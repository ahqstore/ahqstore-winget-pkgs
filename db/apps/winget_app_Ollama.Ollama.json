{"appId":"winget_app_Ollama.Ollama","appShortcutName":"Winget Application","appDisplayName":"Ollama","authorId":"winget","releaseTagName":"winget-0.4.0","downloadUrls":{"0":{"installerType":"WindowsInstallerMsi","asset":"","url":""},"1":{"installerType":"WindowsInstallerExe","asset":"","url":"https://github.com/ollama/ollama/releases/download/v0.4.0/OllamaSetup.exe"}},"install":{"win32":{"assetId":1,"exec":null,"installerArgs":null},"winarm":null,"linux":null,"linuxArm64":null,"linuxArm7":null,"android":null},"displayImages":[],"description":"Get up and running with large language models locally.\n\n\nLlama 3.2 Vision\nOllama 0.4 adds support for Llama 3.2 Vision. After upgrading or downloading Ollama, run:\nollama run llama3.2-vision\nFor the larger, 90B version of the model, run:\nollama run llama3.2-vision:90b\nWhat's changed\n- Support for Llama 3.2 Vision (i.e. Mllama) architecture\n- Improved performance on new generation NVIDIA graphics cards (e.g. RTX 40 series)\n- Sending follow on requests to vision models will now be much faster\n- Fixed issues where stop sequences would not be detected correctly\n- Ollama can now import models from Safetensors without a Modelfile when running ollama create my-model\n- Fixed issue where redirecting output to a file on Windows would cause invalid characters to be written\n- Fixed issue where invalid model data would cause Ollama to error\nFull Changelog: https://github.com/ollama/ollama/compare/v0.3.14...v0.4.0","repo":{"author":"microsoft","repo":"winget-pkgs"},"version":"0.4.0","site":"https://ollama.com/","source":"WinGet","license_or_tos":"MIT","resources":null,"verified":false}